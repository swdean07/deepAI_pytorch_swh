{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qh0h2sFFdKCk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# ✅ 샘플 데이터셋 (문장 예제)\n",
    "corpus = [\n",
    "   \"나는 너를 사랑해\",\n",
    "    \"나는 코딩을 좋아해\",\n",
    "    \"너는 나를 좋아해\",\n",
    "    \"너는 파이썬을 공부해\",\n",
    "    \"우리는 인공지능을 연구해\",\n",
    "    \"딥러닝은 재미있어\",\n",
    "    \"파이썬은 강력해\",\n",
    "    \"나는 자연어처리를 공부해\",\n",
    "]\n",
    "\n",
    "# ✅ 단어 사전 만들기 (Tokenization)\n",
    "# 중복 단어를 제거하여 리스트 생성\n",
    "word_list = list(set(\" \".join(corpus).split()))\n",
    "# 단어를 인덱스로 맵핑, 예시, 단어 : 정수 인덱스 , 구조\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "# 반대로 인덱스를 단어로 맵핑, 예시,  정수 인덱스 : 단어  , 구조\n",
    "idx_dict = {i: w for w, i in word_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ✅ 데이터셋 변환\n",
    "def make_data(corpus):\n",
    "    inputs, targets = [], []\n",
    "    for sentence in corpus:\n",
    "        # 문장을 단어 리스트로 변환\n",
    "        words = sentence.split()\n",
    "        # 다음 단어를 예측하는 문제 구성.\n",
    "        for i in range(len(words) - 1):  # \"I love\" -> \"you\"\n",
    "            # 현재까지의 단어들을 입력으로 사용.\n",
    "            x = [word_dict[w] for w in words[:i+1]]\n",
    "            # 다음 단어를 타겟으로 설정.\n",
    "            y = word_dict[words[i+1]]\n",
    "            inputs.append(x)\n",
    "            targets.append(y)\n",
    "\n",
    "    return inputs, targets\n",
    "\n",
    "inputs, targets = make_data(corpus)\n",
    "\n",
    "# ✅ 패딩 추가 (문장 길이를 맞춤)\n",
    "# 가장 긴 문장의 길이를 기준으로 설정.\n",
    "max_len = max(len(seq) for seq in inputs)\n",
    "# 짧은 문장을 0으로 패딩\n",
    "inputs_padded = [seq + [0] * (max_len - len(seq)) for seq in inputs]\n",
    "# 정답을 텐서로 변환.\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "# ✅ 데이터셋 및 DataLoader 생성\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "dataset = TextDataset(inputs_padded, targets)\n",
    "train_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "vocab_size = len(word_dict)  # 단어 개수\n",
    "embed_size = 10  # 임베딩 차원\n",
    "hidden_size = 16  # RNN 은닉층 크기\n",
    "num_classes = len(word_dict)  # 예측할 단어 개수"
   ],
   "metadata": {
    "id": "vBA5wXf1dO66"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class RNNTextModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_classes):\n",
    "        super(RNNTextModel, self).__init__()\n",
    "        # 단어를 벡터로 변환, 수치화\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)  # 단어 임베딩\n",
    "        # RNN, 계층, 순환 신경망, 은닉층의 값을 다음 층으로 전달.\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])  # 마지막 시점의 RNN 출력을 사용\n",
    "        return out\n",
    "\n",
    "# ✅ 모델 생성\n",
    "model = RNNTextModel(vocab_size, embed_size, hidden_size, num_classes)\n",
    "\n",
    "# ✅ GPU 사용 가능하면 이동\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ✅ 손실 함수 및 최적화 함수 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n"
   ],
   "metadata": {
    "id": "JA-ya6xFdO9i"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "num_epochs = 100\n",
    "print(\"🚀 RNN 모델 학습 시작...\")\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# ✅ 모델 저장\n",
    "model_path = \"/content/drive/MyDrive/busanit501-1234/rnn_korean_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"✅ 학습된 모델이 저장되었습니다: {model_path}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mViO4JjjdPAC",
    "outputId": "cfc00e64-c5e6-4ff5-915d-1db18922d64d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "🚀 RNN 모델 학습 시작...\n",
      "Epoch [10/100], Loss: 0.8826\n",
      "Epoch [20/100], Loss: 0.4593\n",
      "Epoch [30/100], Loss: 0.4012\n",
      "Epoch [40/100], Loss: 0.3711\n",
      "Epoch [50/100], Loss: 0.3696\n",
      "Epoch [60/100], Loss: 0.3681\n",
      "Epoch [70/100], Loss: 0.3545\n",
      "Epoch [80/100], Loss: 0.3639\n",
      "Epoch [90/100], Loss: 0.3621\n",
      "Epoch [100/100], Loss: 0.3470\n",
      "✅ 학습된 모델이 저장되었습니다: /content/drive/MyDrive/busanit501-1234/rnn_korean_model.pth\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# ✅ 저장된 RNN 모델 불러오기\n",
    "def load_model(model_path, vocab_size, embed_size, hidden_size, num_classes):\n",
    "    model = RNNTextModel(vocab_size, embed_size, hidden_size, num_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# ✅ 모델 불러오기\n",
    "loaded_model = load_model(model_path, vocab_size, embed_size, hidden_size, num_classes)\n",
    "print(\"✅ 모델이 성공적으로 불러와졌습니다!\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hVyedZkzdPCq",
    "outputId": "abb02bf9-9de6-4e5d-98e2-02a4396593d9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ 모델이 성공적으로 불러와졌습니다!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_next_word(model, sentence):\n",
    "    \"\"\"\n",
    "    저장된 RNN 모델을 사용하여 주어진 문장의 다음 단어를 예측하는 함수.\n",
    "    \"\"\"\n",
    "    # ✅ 입력 문장을 정수 인코딩\n",
    "    words = sentence.split()\n",
    "    input_seq = [word_dict[w] for w in words if w in word_dict]\n",
    "\n",
    "    # ✅ 패딩 추가 (길이를 맞추기 위해)\n",
    "    input_padded = input_seq + [0] * (max_len - len(input_seq))\n",
    "    input_tensor = torch.tensor([input_padded], dtype=torch.long)\n",
    "\n",
    "    # ✅ 모델 예측\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probabilities = F.softmax(output[0], dim=0)\n",
    "        predicted_idx = torch.argmax(probabilities).item()\n",
    "        confidence = probabilities[predicted_idx].item()\n",
    "\n",
    "    predicted_word = idx_dict[predicted_idx]\n",
    "\n",
    "    print(f\"🔍 입력 문장: '{sentence}'\")\n",
    "    print(f\"📊 예측된 단어: '{predicted_word}'\")\n",
    "    print(f\"✅ 예측 확률: {confidence * 100:.2f}%\")\n",
    "\n",
    "# 🏆 샘플 문장 예측 실행\n",
    "sample_sentence = \"나는 너를\"\n",
    "predict_next_word(loaded_model, sample_sentence)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-uMGRfO3dPFK",
    "outputId": "1d892e93-c738-4f47-9999-6928b3b90857"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "🔍 입력 문장: '나는 너를'\n",
      "📊 예측된 단어: '사랑해'\n",
      "✅ 예측 확률: 99.53%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "VocNLXjcdPHr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "HUAn01PidPKT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "asSHVga-dPMy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "T2J8QYUgdPPa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "na6LO5szdPSL"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
