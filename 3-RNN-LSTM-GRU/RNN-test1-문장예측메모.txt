

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np

# âœ… í•œêµ­ì–´ ë¬¸ì¥ ì˜ˆì œ ë°ì´í„°ì…‹
corpus = [
    "ë‚˜ëŠ” ë„ˆë¥¼ ì‚¬ë‘í•´",
    "ë‚˜ëŠ” ì½”ë”©ì„ ì¢‹ì•„í•´",
    "ë„ˆëŠ” ë‚˜ë¥¼ ì¢‹ì•„í•´",
    "ë„ˆëŠ” íŒŒì´ì¬ì„ ê³µë¶€í•´",
    "ìš°ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ì„ ì—°êµ¬í•´",
    "ë”¥ëŸ¬ë‹ì€ ì¬ë¯¸ìˆì–´",
    "íŒŒì´ì¬ì€ ê°•ë ¥í•´",
    "ë‚˜ëŠ” ìì—°ì–´ì²˜ë¦¬ë¥¼ ê³µë¶€í•´",
]

# âœ… ë‹¨ì–´ ì‚¬ì „ ë§Œë“¤ê¸°
word_list = list(set(" ".join(corpus).split()))
word_dict = {w: i for i, w in enumerate(word_list)}
idx_dict = {i: w for w, i in word_dict.items()}

# âœ… ë°ì´í„°ì…‹ ë³€í™˜
def make_data(corpus):
    inputs, targets = [], []
    for sentence in corpus:
        words = sentence.split()
        for i in range(len(words) - 1):  # "ë‚˜ëŠ” ë„ˆë¥¼" -> "ì‚¬ë‘í•´"
            x = [word_dict[w] for w in words[:i+1]]
            y = word_dict[words[i+1]]
            inputs.append(x)
            targets.append(y)

    return inputs, targets

inputs, targets = make_data(corpus)

# âœ… íŒ¨ë”© ì¶”ê°€ (ë¬¸ì¥ ê¸¸ì´ë¥¼ ë§ì¶¤)
max_len = max(len(seq) for seq in inputs)
inputs_padded = [seq + [0] * (max_len - len(seq)) for seq in inputs]
targets = torch.tensor(targets, dtype=torch.long)

# âœ… ë°ì´í„°ì…‹ ë° DataLoader ìƒì„±
class TextDataset(Dataset):
    def __init__(self, inputs, targets):
        self.inputs = torch.tensor(inputs, dtype=torch.long)
        self.targets = targets

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, idx):
        return self.inputs[idx], self.targets[idx]

dataset = TextDataset(inputs_padded, targets)
train_loader = DataLoader(dataset, batch_size=2, shuffle=True)

vocab_size = len(word_dict)  # ë‹¨ì–´ ê°œìˆ˜
embed_size = 10  # ì„ë² ë”© ì°¨ì›
hidden_size = 16  # RNN ì€ë‹‰ì¸µ í¬ê¸°
num_classes = len(word_dict)  # ì˜ˆì¸¡í•  ë‹¨ì–´ ê°œìˆ˜
2ï¸ RNN ëª¨ë¸ ì •ì˜



class RNNTextModel(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size, num_classes):
        super(RNNTextModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)  # ë‹¨ì–´ ì„ë² ë”©
        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        x = self.embedding(x)
        out, _ = self.rnn(x)
        out = self.fc(out[:, -1, :])  # ë§ˆì§€ë§‰ ì‹œì ì˜ RNN ì¶œë ¥ì„ ì‚¬ìš©
        return out

# âœ… ëª¨ë¸ ìƒì„±
model = RNNTextModel(vocab_size, embed_size, hidden_size, num_classes)

# âœ… GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ì´ë™
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# âœ… ì†ì‹¤ í•¨ìˆ˜ ë° ìµœì í™” í•¨ìˆ˜ ì„¤ì •
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)
3ï¸ ëª¨ë¸ í•™ìŠµ ë° ì €ì¥



num_epochs = 100
print("ğŸš€ RNN ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
for epoch in range(num_epochs):
    running_loss = 0.0
    for i, (inputs, labels) in enumerate(train_loader):
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    if (epoch + 1) % 10 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}")

# âœ… ëª¨ë¸ ì €ì¥
model_path = "/content/drive/MyDrive/busanit501-1234/rnn_korean_model.pth"
torch.save(model.state_dict(), model_path)
print(f"âœ… í•™ìŠµëœ ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {model_path}")
4ï¸ ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°



# âœ… ì €ì¥ëœ RNN ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
def load_model(model_path, vocab_size, embed_size, hidden_size, num_classes):
    model = RNNTextModel(vocab_size, embed_size, hidden_size, num_classes)
    model.load_state_dict(torch.load(model_path, map_location=torch.device("cpu")))
    model.eval()
    return model

# âœ… ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
loaded_model = load_model(model_path, vocab_size, embed_size, hidden_size, num_classes)
print("âœ… ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™€ì¡ŒìŠµë‹ˆë‹¤!")
5ï¸ ìƒ˜í”Œ ë¬¸ì¥ ì˜ˆì¸¡ (ì˜ˆì¸¡ ë‹¨ì–´ ë° ì •í™•ë„ ì¶œë ¥)



import torch.nn.functional as F

def predict_next_word(model, sentence):
    """
    ì €ì¥ëœ RNN ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ ë¬¸ì¥ì˜ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜.
    """
    # âœ… ì…ë ¥ ë¬¸ì¥ì„ ì •ìˆ˜ ì¸ì½”ë”©
    words = sentence.split()
    input_seq = [word_dict[w] for w in words if w in word_dict]

    # âœ… íŒ¨ë”© ì¶”ê°€ (ê¸¸ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•´)
    input_padded = input_seq + [0] * (max_len - len(input_seq))
    input_tensor = torch.tensor([input_padded], dtype=torch.long)

    # âœ… ëª¨ë¸ ì˜ˆì¸¡
    with torch.no_grad():
        output = model(input_tensor)
        probabilities = F.softmax(output[0], dim=0)
        predicted_idx = torch.argmax(probabilities).item()
        confidence = probabilities[predicted_idx].item()

    predicted_word = idx_dict[predicted_idx]

    print(f"ğŸ” ì…ë ¥ ë¬¸ì¥: '{sentence}'")
    print(f"ğŸ“Š ì˜ˆì¸¡ëœ ë‹¨ì–´: '{predicted_word}'")
    print(f"âœ… ì˜ˆì¸¡ í™•ë¥ : {confidence * 100:.2f}%")

# ğŸ† ìƒ˜í”Œ ë¬¸ì¥ ì˜ˆì¸¡ ì‹¤í–‰
sample_sentence = "ë‚˜ëŠ” ë„ˆë¥¼"
predict_next_word(loaded_model, sample_sentence)