위 그래프는 대표적인 활성화 함수들의 형태를 보여줍니다.

ReLU (Rectified Linear Unit)

음수 값은 0으로 변환하고, 양수 값은 그대로 유지합니다.
비선형성을 추가하면서도 계산이 효율적이고, 기울기 소실(Vanishing Gradient) 문제를 완화합니다.

Sigmoid

출력이 0과 1 사이의 값으로 제한되며, 이진 분류(Binary Classification)에 사용됩니다.
그러나 큰 값에서 기울기가 매우 작아지는 기울기 소실 문제가 발생할 수 있습니다.

Tanh (Hyperbolic Tangent)

출력이 -1과 1 사이의 값으로 제한되며, Sigmoid보다 중심이 0에 가까워 학습이 더 빠르게 진행될 수 있습니다.
하지만 극단적인 값에서 여전히 기울기 소실 문제가 발생할 수 있습니다.

Softmax

다중 클래스 분류(Multi-class Classification)에서 사용되며, 입력값을 확률 분포 형태로 변환합니다.
각 클래스에 대한 확률을 출력하며, 총합이 1이 됩니다.
각 함수가 딥러닝에서 어떻게 사용되는지 추가적으로 궁금한 점이 있으면 질문해주세요! 🚀 ​